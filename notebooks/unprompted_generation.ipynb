{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unprompted_generation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e7aee9eb733c4537b0e78795bdf582a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b591e65dc6c84fdbbf27a7d942a2f414",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0f9f2c06a77a40b0908db938e04e5e6f",
              "IPY_MODEL_0a7987c7fe6f4e8e8121618a0552cfd7"
            ]
          }
        },
        "b591e65dc6c84fdbbf27a7d942a2f414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "0f9f2c06a77a40b0908db938e04e5e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1d657bf3a99a4657a6e198812dab54b7",
            "_dom_classes": [],
            "description": "Generation: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_648b9e882a1543bbbbf041d34daddaa6"
          }
        },
        "0a7987c7fe6f4e8e8121618a0552cfd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_510c828c02d44eed92b58f9c9dd4ad8f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10000/10000 [2:47:09&lt;00:00,  1.00s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2082377cdc1541bc8b6e50a403558b9d"
          }
        },
        "1d657bf3a99a4657a6e198812dab54b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "648b9e882a1543bbbbf041d34daddaa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "510c828c02d44eed92b58f9c9dd4ad8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2082377cdc1541bc8b6e50a403558b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d2c18127d5a46e59f652c1d52b1020c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3228b392207947989b1eecf243583628",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2e3fb1049d94c70a93e4acb9b3d7ef3",
              "IPY_MODEL_98c378636d7b4099a5fcf2ab530e85dd"
            ]
          }
        },
        "3228b392207947989b1eecf243583628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2e3fb1049d94c70a93e4acb9b3d7ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_06f3b4a7d7f84571a891c27c8bb95617",
            "_dom_classes": [],
            "description": "Collating files: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd7e3533fc9d4f348a8aaa7cce4b78a1"
          }
        },
        "98c378636d7b4099a5fcf2ab530e85dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e6c1556b3af546efb9b46e11b2b518dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10000/10000 [00:19&lt;00:00, 502.30it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09dac1e26ebc421e8c1a367324d88e5c"
          }
        },
        "06f3b4a7d7f84571a891c27c8bb95617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd7e3533fc9d4f348a8aaa7cce4b78a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6c1556b3af546efb9b46e11b2b518dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09dac1e26ebc421e8c1a367324d88e5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuzWFMrIqDPR",
        "outputId": "d813de13-80ff-45a7-cee9-2b5627fcebe7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "MyDrive  Shareddrives\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s47_02lDqM6E",
        "outputId": "1c7f5a42-d7e7-42ad-99b4-4d5526e83da5"
      },
      "source": [
        "import os\n",
        "os.chdir('/gdrive/My Drive/517finalP')\n",
        "!wget https://ai2-public-datasets.s3.amazonaws.com/realtoxicityprompts/realtoxicityprompts-data.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-10 23:01:40--  https://ai2-public-datasets.s3.amazonaws.com/realtoxicityprompts/realtoxicityprompts-data.tar.gz\n",
            "Resolving ai2-public-datasets.s3.amazonaws.com (ai2-public-datasets.s3.amazonaws.com)... 52.218.221.91\n",
            "Connecting to ai2-public-datasets.s3.amazonaws.com (ai2-public-datasets.s3.amazonaws.com)|52.218.221.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3715984281 (3.5G) [binary/octet-stream]\n",
            "Saving to: ‘realtoxicityprompts-data.tar.gz’\n",
            "\n",
            "realtoxicityprompts   1%[                    ]  63.99M  31.1MB/s               ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MJhRGibuWhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a6d7d9-a0c2-4442-94c6-309cd8602e69"
      },
      "source": [
        "import os\n",
        "os.chdir('/gdrive/My Drive/517finalP')\n",
        "!tar -zxvf realtoxicityprompts-data.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "realtoxicityprompts-data/\n",
            "realtoxicityprompts-data/README.md\n",
            "realtoxicityprompts-data/prompts.jsonl\n",
            "realtoxicityprompts-data/metadata/\n",
            "realtoxicityprompts-data/metadata/reddit_banned_subreddits.jsonl\n",
            "realtoxicityprompts-data/metadata/media_bias_fact_check.jsonl\n",
            "realtoxicityprompts-data/metadata/reddit_subscriber_counts.jsonl\n",
            "realtoxicityprompts-data/metadata/openwebtext_meta.jsonl\n",
            "realtoxicityprompts-data/generations/\n",
            "realtoxicityprompts-data/generations/eos/\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_gpt2.jsonl\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_gpt2_dapttoxic.jsonl\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_gpt1.jsonl\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_gpt3_davinci.jsonl\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_ctrl.jsonl\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_gpt2_atcon.jsonl\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_gpt2_wordfilter.jsonl\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_gpt2_vocabshift.jsonl\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_ctrl_wiki.jsonl\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_gpt2_medium.jsonl\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_gpt2_daptnontoxic.jsonl\n",
            "realtoxicityprompts-data/generations/eos/eos_gens_gpt2_pplm.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_gpt2_medium.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_gpt3_davinci.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_gpt2_daptnontoxic.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_gpt2_dapttoxic.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_gpt1.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_ctrl_wiki.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_gpt2_pplm.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_gpt2.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_gpt2_vocabshift.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_ctrl.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_gpt2_atcon.jsonl\n",
            "realtoxicityprompts-data/generations/prompted/prompted_gens_gpt2_wordfilter.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yxNJc-fAUFI"
      },
      "source": [
        "promptsPath = open('/gdrive/My Drive/517finalP/realtoxicityprompts-data/prompts.jsonl', 'r')\n",
        "count = 0;\n",
        "f = open('/gdrive/My Drive/517finalP/realtoxicityprompts-data/promptsSmall.jsonl', 'w')\n",
        "l = \"\"\n",
        "for line in promptsPath:\n",
        "  if count == 1:\n",
        "    break;\n",
        "  else:\n",
        "    l = line\n",
        "  count = count + 1\n",
        "f.write(l)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCeRJo6sDU9I"
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import TypeVar, Iterable, List, Sequence, Union, Any\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "T = TypeVar('T')\n",
        "\n",
        "\n",
        "def batchify(data: Iterable[T], batch_size: int) -> Iterable[List[T]]:\n",
        "    assert batch_size > 0\n",
        "\n",
        "    batch = []\n",
        "    for item in data:\n",
        "        # Yield next batch\n",
        "        if len(batch) == batch_size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "\n",
        "        batch.append(item)\n",
        "\n",
        "    # Yield last un-filled batch\n",
        "    if len(batch) != 0:\n",
        "        yield batch\n",
        "\n",
        "\n",
        "def first(iterable):\n",
        "    return next(iter(iterable), None)\n",
        "\n",
        "\n",
        "def save_gpt2_training_data(corpus: Sequence[str], out_file: Union[str, Path], eos_token='<|endoftext|>'):\n",
        "    with open(out_file, 'a') as f:\n",
        "        for i, text in enumerate(tqdm(corpus, desc='Saving training data')):\n",
        "            print(text, file=f, end='')\n",
        "            if i != len(corpus) - 1:\n",
        "                print(eos_token, file=f, end='')\n",
        "\n",
        "\n",
        "def set_seed(seed, n_gpu):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def load_jsonl(file: Union[str, Path]) -> Iterable[Any]:\n",
        "    with open(file) as f:\n",
        "        for line in f:\n",
        "            yield json.loads(line)\n",
        "\n",
        "\n",
        "def load_cache(file: Path):\n",
        "    if file.exists():\n",
        "        with file.open() as f:\n",
        "            for line in tqdm(f, desc=f'Loading cache from {file}'):\n",
        "                yield json.loads(line)\n",
        "\n",
        "\n",
        "def big_flat_jsonl_to_csv(jsonl_file, csv_file, chunksize=100_000, header=True):\n",
        "    chunks = pd.read_json(jsonl_file, lines=True, chunksize=chunksize)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk.to_csv(csv_file, header=header, mode='a', index=False)\n",
        "        header = False  # disable header after first rows are printed\n",
        "\n",
        "\n",
        "def reorder_csv(csv_file_in, csv_file_out, columns, chunksize=100_000):\n",
        "    chunks = pd.read_csv(csv_file_in, chunksize=chunksize)\n",
        "\n",
        "    header = True\n",
        "    for chunk in chunks:\n",
        "        chunk.to_csv(csv_file_out, header=header, mode='a', index=False, columns=columns)\n",
        "        header = False  # disable header after first rows are printed\n",
        "\n",
        "\n",
        "def make_corpus_iter(corpus_dir: Path):\n",
        "    files = sorted([file for file in corpus_dir.iterdir() if file.suffix == '.joblib'])\n",
        "\n",
        "    i = 0\n",
        "    for file in files:\n",
        "        docs = joblib.load(file)\n",
        "\n",
        "        # Load filenames or ids\n",
        "        filenames_file = file.with_name(f'{file.stem}_filenames.txt')\n",
        "        doc_ids = (\n",
        "            filenames_file.read_text().split()\n",
        "            if filenames_file.exists()\n",
        "            else map(lambda idx: f'{file.stem}-{idx}', range(len(docs)))\n",
        "        )\n",
        "\n",
        "        print(\"Loading file:\", file)\n",
        "        for doc_id, doc in zip(doc_ids, docs):\n",
        "            # Yield name and doc\n",
        "            yield doc_id, doc\n",
        "            i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV0aXfPLCGL7",
        "outputId": "9bd2b2e9-5091-49a6-81e1-1152074b86ad"
      },
      "source": [
        "import json\n",
        "import logging\n",
        "import math\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Iterable, List\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "!pip install transformers\n",
        "from transformers.pipelines import pipeline\n",
        "\n",
        "logging.disable(logging.CRITICAL)  # Disable logging from transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 22.3MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=8b7287407e92ef3bf4319094a4f6a0730e0f96c150d7a196ed5babcb74ba0cb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STU_m5_yDnr5"
      },
      "source": [
        "from pathlib import Path\n",
        "from typing import Union, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2PreTrainedModel\n",
        "from transformers import top_k_top_p_filtering\n",
        "\n",
        "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
        "\n",
        "\n",
        "def adjust_length_to_model(length, max_sequence_length):\n",
        "    if length < 0 < max_sequence_length:\n",
        "        length = max_sequence_length\n",
        "    elif 0 < max_sequence_length < length:\n",
        "        length = max_sequence_length  # No generation bigger than model size\n",
        "    elif length < 0:\n",
        "        length = MAX_LENGTH  # avoid infinite loop\n",
        "    return length\n",
        "\n",
        "\n",
        "# TODO: convert to HuggingFace pipeline\n",
        "class GPT2Generation:\n",
        "    STOP_TOKEN = \"<|endoftext|>\"\n",
        "\n",
        "    def __init__(self, model: Union[str, Path, GPT2PreTrainedModel] = 'gpt2', tokenizer: str = 'gpt2', seed: int = 42):\n",
        "        # Set up device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "        set_seed(seed, n_gpu)\n",
        "\n",
        "        # Set up model\n",
        "        if isinstance(model, Path) or isinstance(model, str):\n",
        "            model = GPT2LMHeadModel.from_pretrained(str(model))\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        # Set up tokenizer\n",
        "        # IMPORTANT: Note that setting the pad token like this in the constructor gives the pad_token the\n",
        "        # pad_token_id = 50256, which normally belongs to the <EOS> token_id in GPT2. This is a very ugly\n",
        "        # way that works at the moment of setting the pad_token_id to the <EOS> token that is already\n",
        "        # included in the vocab size.\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(tokenizer, pad_token=self.STOP_TOKEN)\n",
        "        assert self.tokenizer.eos_token_id == self.tokenizer.pad_token_id\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'<GPT2Generator model_name_or_path=\"{self.model}\">'\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.generate(*args, **kwargs)\n",
        "\n",
        "    def generate(self,\n",
        "                 prompt: Union[str, List[str]],\n",
        "                 max_len: int = 20,\n",
        "                 sample: bool = True,\n",
        "                 k: int = 0,\n",
        "                 p: float = 0.9,\n",
        "                 temperature: float = 1.0,\n",
        "                 bad_words_ids: List[List[int]] = None,\n",
        "                 **model_kwargs) -> List[str]:\n",
        "        if isinstance(prompt, str):\n",
        "            prompt = [prompt]\n",
        "\n",
        "        encodings_dict = self.tokenizer.batch_encode_plus(prompt, pad_to_max_length=True, return_tensors='pt')\n",
        "\n",
        "        input_ids = encodings_dict['input_ids'].to(self.device)\n",
        "        attention_mask = encodings_dict['attention_mask'].to(self.device)\n",
        "        batch_size, input_seq_len = input_ids.shape\n",
        "\n",
        "        position_ids = attention_mask.cumsum(dim=1) - 1\n",
        "        unfinished_sents = torch.ones(batch_size, dtype=torch.long, device=self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for step in range(max_len):\n",
        "                pp = self.model(input_ids, attention_mask=attention_mask, position_ids=position_ids,\n",
        "                                          **model_kwargs)\n",
        "                logits = pp.logits\n",
        "                # in the first decoding step, we want to use the 'real' last position for each sentence\n",
        "                if step == 0:\n",
        "                    last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n",
        "                    next_token_logits = logits[range(batch_size), last_non_masked_idx, :]\n",
        "                else:\n",
        "                    next_token_logits = logits[:, -1, :]\n",
        "\n",
        "                if sample:\n",
        "                    # Temperature (higher temperature => more likely to sample low probability tokens)\n",
        "                    if temperature != 1.0:\n",
        "                        next_token_logits = next_token_logits / temperature\n",
        "                    # Top-p/top-k filtering\n",
        "                    next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=k, top_p=p)\n",
        "                    # Sample\n",
        "                    probs = F.softmax(next_token_logits, dim=-1)\n",
        "                    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "                else:\n",
        "                    # Greedy decoding\n",
        "                    next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "                # either append a padding token here if <EOS> has been seen or append next token\n",
        "                tokens_to_add = next_tokens * unfinished_sents + self.tokenizer.pad_token_id * (1 - unfinished_sents)\n",
        "\n",
        "                # this updates which sentences have not seen an EOS token so far\n",
        "                # if one EOS token was seen the sentence is finished\n",
        "                eos_in_sents = tokens_to_add == self.tokenizer.eos_token_id\n",
        "                unfinished_sents.mul_((~eos_in_sents).long())\n",
        "\n",
        "                # stop when there is an EOS in each sentence\n",
        "                if unfinished_sents.max() == 0:\n",
        "                    break\n",
        "\n",
        "                # Update input_ids, attention_mask and position_ids\n",
        "                input_ids = torch.cat([input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
        "                attention_mask = torch.cat([attention_mask, attention_mask.new_ones((batch_size, 1))], dim=1)\n",
        "                position_ids = torch.cat([position_ids, (position_ids[:, -1] + 1).unsqueeze(-1)], dim=1)\n",
        "\n",
        "        decoded_outputs = [self.tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "                           for output in input_ids[:, input_seq_len:]]\n",
        "        return decoded_outputs\n",
        "\n",
        "    def generate_multiple(self,\n",
        "                          prompt: str,\n",
        "                          max_len: int = 20,\n",
        "                          temperature: float = 1.0,\n",
        "                          k: int = 0,\n",
        "                          p: float = 0.9,\n",
        "                          num_return_sequences: int = 1,\n",
        "                          sample: bool = True,\n",
        "                          repetition_penalty: float = 1.0):\n",
        "        max_len = adjust_length_to_model(max_len, max_sequence_length=self.model.config.max_position_embeddings)\n",
        "\n",
        "        encoded_prompt = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "        encoded_prompt = encoded_prompt.to(self.device)\n",
        "\n",
        "        prompt_len = len(encoded_prompt[0])\n",
        "\n",
        "        output_sequences = self.model.generate(\n",
        "            input_ids=encoded_prompt,\n",
        "            max_length=max_len + prompt_len,\n",
        "            temperature=temperature,\n",
        "            top_k=k,\n",
        "            top_p=p,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            do_sample=sample,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "        )\n",
        "\n",
        "        # Remove the batch dimension when returning multiple sequences\n",
        "        if len(output_sequences.shape) > 2:\n",
        "            output_sequences.squeeze_()\n",
        "\n",
        "        decoded_outputs = []\n",
        "        for output in output_sequences:\n",
        "            output = output[prompt_len:]\n",
        "            try:\n",
        "                stop_index = [i for i, x in enumerate(output) if x == self.tokenizer.eos_token_id][0]\n",
        "            except IndexError:\n",
        "                stop_index = None\n",
        "            output = output[:stop_index]\n",
        "            decoded_outputs.append(self.tokenizer.decode(output, clean_up_tokenization_spaces=True))\n",
        "\n",
        "        return decoded_outputs\n",
        "\n",
        "\n",
        "def test_generate():\n",
        "    generator = GPT2Generation()\n",
        "    prompt = [\n",
        "        '<|endoftext|>in this paper we',\n",
        "        '<|endoftext|>we are trying to',\n",
        "        '<|endoftext|>The purpose of this workshop is to check whether we can'\n",
        "    ]\n",
        "    out = generator.generate(prompt)\n",
        "    print(*out, sep='\\n')\n",
        "\n",
        "\n",
        "def test_generate_multiple():\n",
        "    generator = GPT2Generation()\n",
        "    prompt = 'in this paper we'\n",
        "    out = generator.generate_multiple(prompt)\n",
        "    print(*out, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBxbsZM7HW1I"
      },
      "source": [
        "def _pipeline_helper(prompts: pd.Series,\n",
        "                     model_name_or_path: str,\n",
        "                     max_len: int,\n",
        "                     num_samples: int,\n",
        "                     out_file: Path,\n",
        "                     **generate_kwargs):\n",
        "    # Load cached generations\n",
        "    num_cached_generations = 0\n",
        "    for generation in load_cache(out_file):\n",
        "        yield generation\n",
        "        num_cached_generations += 1\n",
        "    assert num_cached_generations % num_samples == 0\n",
        "\n",
        "    # Remove prompts that have already been generated with\n",
        "    prompts = prompts[num_cached_generations // num_samples:]\n",
        "    if prompts.empty:\n",
        "        return\n",
        "\n",
        "    # Setup model\n",
        "    generator = pipeline('text-generation', model=model_name_or_path, device=0)\n",
        "    print(\"Created pipeline with model:\", generator.model.__class__.__name__)\n",
        "\n",
        "    # Generate with prompts\n",
        "    for prompt in tqdm(prompts, desc='Generation', dynamic_ncols=True):\n",
        "        # Generate\n",
        "        # FIXME: this is a hack\n",
        "        ctx_len = len(generator.tokenizer.tokenize(prompt))\n",
        "        try:\n",
        "            batch = generator(prompt,\n",
        "                              num_return_sequences=num_samples,\n",
        "                              clean_up_tokenization_spaces=True,\n",
        "                              do_sample=True,\n",
        "                              top_k=0,\n",
        "                              top_p=0.9,\n",
        "                              max_length=ctx_len + max_len,\n",
        "                              return_prompt=False,\n",
        "                              **generate_kwargs)\n",
        "            batch = map(lambda g: g['generated_text'][len(prompt):], batch)\n",
        "        except RuntimeError as e:\n",
        "            print(\"Error during generation with prompt:\", prompt)\n",
        "            print(e)\n",
        "            print(\"Emptying CUDA cache and continuing...\")\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            batch = [\"GENERATION_ERROR_CUDA\"] * num_samples\n",
        "\n",
        "        for generation in batch:\n",
        "            with out_file.open('a') as f:\n",
        "                print(json.dumps(generation), file=f)\n",
        "            yield generation\n",
        "\n",
        "\n",
        "def openai_gpt(prompts: pd.Series,\n",
        "               max_len: int,\n",
        "               num_samples: int,\n",
        "               model_name_or_path: str,\n",
        "               out_file: Path,\n",
        "               **generate_kwargs):\n",
        "    yield from _pipeline_helper(prompts=prompts,\n",
        "                                model_name_or_path=model_name_or_path,\n",
        "                                max_len=max_len,\n",
        "                                num_samples=num_samples,\n",
        "                                out_file=out_file,\n",
        "                                **generate_kwargs)\n",
        "\n",
        "def ctrl(prompts: pd.Series,\n",
        "         max_len: int,\n",
        "         num_samples: int,\n",
        "         ctrl_code: str,\n",
        "         model_name_or_path: str,\n",
        "         out_file: Path,\n",
        "         **generate_kwargs) -> Iterable[str]:\n",
        "    # Prepend CTRL code to prompts\n",
        "    prompts = ctrl_code + \" \" + prompts\n",
        "    print(prompts)\n",
        "\n",
        "    yield from _pipeline_helper(prompts=prompts,\n",
        "                                model_name_or_path=model_name_or_path,\n",
        "                                max_len=max_len,\n",
        "                                num_samples=num_samples,\n",
        "                                out_file=out_file,\n",
        "                                **generate_kwargs)\n",
        "\n",
        "def _gpt2_helper(prompts: pd.Series,\n",
        "                 max_len: int,\n",
        "                 num_samples: int,\n",
        "                 batch_size: int,\n",
        "                 generator: GPT2Generation,\n",
        "                 out_file: Path,\n",
        "                 **generate_kwargs):\n",
        "    # Repeat prompts\n",
        "    prompts = prompts.repeat(num_samples)\n",
        "\n",
        "    # Resume generation\n",
        "    num_cached_generations = 0\n",
        "    for generation in load_cache(out_file):\n",
        "        yield generation\n",
        "        num_cached_generations += 1\n",
        "\n",
        "    # Generate with prompts\n",
        "    prompts = prompts[num_cached_generations:]\n",
        "    for prompt in tqdm(batchify(prompts, batch_size),\n",
        "                       total=math.ceil(len(prompts) / batch_size),\n",
        "                       desc=f'GPT-2 Generation',\n",
        "                       dynamic_ncols=True,\n",
        "                       postfix={'batch_size': batch_size}):\n",
        "        # Generate\n",
        "        try:\n",
        "            batch = generator.generate(prompt, max_len, **generate_kwargs)\n",
        "        except RuntimeError as e:\n",
        "            print(\"Error during generation with prompt:\", prompt)\n",
        "            print(e)\n",
        "            print(\"Emptying CUDA cache and retrying...\")\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            batch = [\"GENERATION_ERROR_CUDA\"] * len(prompt)\n",
        "\n",
        "        for generation in batch:\n",
        "            with out_file.open('a') as f:\n",
        "                print(json.dumps(generation), file=f)\n",
        "            yield generation\n",
        "\n",
        "def gpt2(prompts: pd.Series,\n",
        "         max_len: int,\n",
        "         num_samples: int,\n",
        "         batch_size: int,\n",
        "         model_name_or_path: str,\n",
        "         out_file: Path,\n",
        "         **generate_kwargs) -> Iterable[str]:\n",
        "    # Setup model\n",
        "    generator = GPT2Generation(model_name_or_path)\n",
        "\n",
        "    yield from _gpt2_helper(prompts=prompts,\n",
        "                            max_len=max_len,\n",
        "                            num_samples=num_samples,\n",
        "                            batch_size=batch_size,\n",
        "                            generator=generator,\n",
        "                            out_file=out_file,\n",
        "                            **generate_kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJvM8I7ZIiAV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecn3bgx0McOx"
      },
      "source": [
        "# Project-level constants, including API keys and directories\n",
        "# Note: importing this file has the side effect of loading a configuration file\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "\n",
        "##############################\n",
        "# Config\n",
        "##############################\n",
        "#CONFIG_FILE = Path('config.yml')\n",
        "PERSPECTIVE_API_KEY = 'AIzaSyDTAEdPqzszNrn0OTvCfTCQstGOFuouS8Q'\n",
        "\n",
        "#try:\n",
        "   # with open(CONFIG_FILE) as f:\n",
        " #       config = yaml.load(f, Loader=yaml.FullLoader)\n",
        " #   PERSPECTIVE_API_KEY = config['perspective']\n",
        "#except FileNotFoundError:\n",
        "  #  print('No config file found. API keys will not be loaded.')\n",
        "\n",
        "##############################\n",
        "# Paths\n",
        "##############################\n",
        "SHARED_DIR = Path('/data/language-model-toxicity')\n",
        "DATA_DIR = SHARED_DIR / 'data'\n",
        "OUTPUT_DIR = Path('output')\n",
        "\n",
        "OPENWEBTEXT_DB = DATA_DIR / 'openwebtext-perspective.db'\n",
        "WEBTEXT_DB = DATA_DIR / 'webtext.db'\n",
        "\n",
        "# TODO: remove\n",
        "TEXTS_DIR = DATA_DIR / 'openwebtext'\n",
        "\n",
        "##############################\n",
        "# Perspective API\n",
        "##############################\n",
        "PERSPECTIVE_API_LEN_LIMIT = 20480\n",
        "\n",
        "# All attributes can be found here:\n",
        "# https://github.com/conversationai/perspectiveapi/blob/master/2-api/models.md\n",
        "PERSPECTIVE_API_ATTRIBUTES = (\n",
        "    'TOXICITY',\n",
        "    'SEVERE_TOXICITY',\n",
        "    'IDENTITY_ATTACK',\n",
        "    'INSULT',\n",
        "    'THREAT',\n",
        "    'PROFANITY',\n",
        "    'SEXUALLY_EXPLICIT',\n",
        "    'FLIRTATION'\n",
        ")\n",
        "PERSPECTIVE_API_ATTRIBUTES_LOWER = tuple(a.lower() for a in PERSPECTIVE_API_ATTRIBUTES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZipXqIpMUq9"
      },
      "source": [
        "import collections\n",
        "import json\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Union, Optional, Tuple, Dict, Any, Iterable\n",
        "\n",
        "from googleapiclient import discovery\n",
        "from googleapiclient.errors import HttpError\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def unpack_scores(response_json: dict) -> Optional[Tuple[dict, dict]]:\n",
        "    if not response_json:\n",
        "        return None\n",
        "\n",
        "    attribute_scores = response_json['attributeScores'].items()\n",
        "\n",
        "    summary_scores = {}\n",
        "    span_scores = {}\n",
        "    for attribute, scores in attribute_scores:\n",
        "        attribute = attribute.lower()\n",
        "\n",
        "        # Save summary score\n",
        "        assert scores['summaryScore']['type'] == 'PROBABILITY'\n",
        "        summary_scores[attribute] = scores['summaryScore']['value']\n",
        "\n",
        "        # Save span scores\n",
        "        for span_score_dict in scores['spanScores']:\n",
        "            assert span_score_dict['score']['type'] == 'PROBABILITY'\n",
        "            span = (span_score_dict['begin'], span_score_dict['end'])\n",
        "            span_scores.setdefault(span, {})[attribute] = span_score_dict['score']['value']\n",
        "\n",
        "    return summary_scores, span_scores\n",
        "\n",
        "\n",
        "class PerspectiveAPI:\n",
        "    def __init__(self, api_key: str = PERSPECTIVE_API_KEY, rate_limit: int = 25):\n",
        "        self.service = self._make_service(api_key)\n",
        "        self.last_request_time = -1  # satisfies initial condition\n",
        "        self.rate_limit = rate_limit\n",
        "        self.next_uid = 0\n",
        "\n",
        "    def request(self, texts: Union[str, List[str]]) -> List[Tuple[Optional[Dict[str, Any]], Optional[HttpError]]]:\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        # Rate limit to 1 batch request per second\n",
        "        assert len(texts) <= self.rate_limit\n",
        "        time_since_last_request = time.time() - self.last_request_time\n",
        "        if time_since_last_request < 1:\n",
        "            time.sleep(1 - time_since_last_request)\n",
        "        self.last_request_time = time.time()\n",
        "\n",
        "        # Keys guaranteed in insertion order (Python 3.7+)\n",
        "        responses = {str(uid): None for uid in range(self.next_uid, self.next_uid + len(texts))}\n",
        "        self.next_uid += len(texts)\n",
        "\n",
        "        def response_callback(request_id, response, exception):\n",
        "            nonlocal responses\n",
        "            responses[request_id] = (response, exception)\n",
        "\n",
        "        # Make API request\n",
        "        batch_request = self.service.new_batch_http_request()\n",
        "        for uid, text in zip(responses.keys(), texts):\n",
        "            batch_request.add(self._make_request(text, self.service), callback=response_callback, request_id=uid)\n",
        "        batch_request.execute()\n",
        "\n",
        "        return list(responses.values())\n",
        "\n",
        "    def request_bulk(self,\n",
        "                     corpus: Union[Iterable[str], Iterable[Tuple[str, str]]],\n",
        "                     output_file: Union[str, Path],\n",
        "                     pbar: tqdm = None):\n",
        "        # Check for output file\n",
        "        output_file = Path(output_file)\n",
        "        assert not output_file.exists()\n",
        "\n",
        "        # Set up progress bar\n",
        "        if not pbar:\n",
        "            total = len(corpus) if isinstance(corpus, collections.abc.Sequence) else None\n",
        "            pbar = tqdm(total=total, dynamic_ncols=True)\n",
        "        pbar.set_description(f'Perspective API')\n",
        "\n",
        "        i = 0\n",
        "        num_failures = 0\n",
        "        with output_file.open('a') as f:\n",
        "            for batch in batchify(corpus, self.rate_limit):\n",
        "                request_ids = None\n",
        "                if isinstance(batch[0], tuple):\n",
        "                    request_ids, batch = zip(*batch)\n",
        "\n",
        "                for j, (response, exception) in enumerate(self.request(batch)):\n",
        "                    response_dict = {\n",
        "                        'request_id': request_ids[j] if request_ids else i,\n",
        "                        'response': response,\n",
        "                        'error': str(exception) if exception else None\n",
        "                    }\n",
        "\n",
        "                    # Save response\n",
        "                    json.dump(response_dict, f)\n",
        "                    f.write('\\n')\n",
        "\n",
        "                    if exception:\n",
        "                        num_failures += 1\n",
        "\n",
        "                i += len(batch)\n",
        "                pbar.update(len(batch))\n",
        "                pbar.set_postfix(failures=num_failures, rate_limt=self.rate_limit)\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_service(api_key: str):\n",
        "        # Generate API client object dynamically based on service name and version\n",
        "        return discovery.build('commentanalyzer', 'v1alpha1', developerKey=api_key)\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_request(text: str, service):\n",
        "        analyze_request = {\n",
        "            'comment': {'text': text},\n",
        "            'requestedAttributes': {attr: {} for attr in PERSPECTIVE_API_ATTRIBUTES},\n",
        "            'spanAnnotations': True,\n",
        "        }\n",
        "        return service.comments().analyze(body=analyze_request)\n",
        "\n",
        "\n",
        "class PerspectiveWorker:\n",
        "    SENTINEL = 'STOP'\n",
        "\n",
        "    def __init__(self, out_file: Path, total: int, rate_limit: int):\n",
        "        if not rate_limit:\n",
        "            print(\"Disabling Perspective API (rps is 0)\")\n",
        "            self.enabled = False\n",
        "            return\n",
        "        self.enabled = True\n",
        "\n",
        "        self.requests_handled = set()\n",
        "        for response in load_cache(out_file):\n",
        "            self.requests_handled.add(response['request_id'])\n",
        "        total -= len(self.requests_handled)\n",
        "\n",
        "        # Setup worker thread\n",
        "        self.task_queue = mp.Queue()\n",
        "        self.process = mp.Process(target=self.perspective_worker,\n",
        "                                  args=(self.task_queue, out_file, total, rate_limit))\n",
        "        self.process.start()\n",
        "\n",
        "    def __call__(self, request_id: str, text: str):\n",
        "        if not self.enabled:\n",
        "            return\n",
        "\n",
        "        if request_id not in self.requests_handled:\n",
        "            self.task_queue.put((request_id, text))\n",
        "\n",
        "    def stop(self):\n",
        "        if not self.enabled:\n",
        "            return\n",
        "\n",
        "        print(\"Waiting for Perspective to finish...\")\n",
        "        self.task_queue.put(self.SENTINEL)\n",
        "        self.process.join()\n",
        "\n",
        "    @classmethod\n",
        "    def perspective_worker(cls, queue: mp.Queue, responses_file: Path, total: int, rate_limit: int):\n",
        "        queue_iter = iter(queue.get, cls.SENTINEL)\n",
        "        api = PerspectiveAPI(rate_limit=rate_limit)\n",
        "        pbar = tqdm(total=total, dynamic_ncols=True, position=1)\n",
        "        api.request_bulk(queue_iter, output_file=responses_file, pbar=pbar)\n",
        "\n",
        "\n",
        "def test_perspective_api():\n",
        "    api = PerspectiveAPI()\n",
        "\n",
        "    text_success = \"Testing\"\n",
        "    text_error = 'x' * (20480 + 1)\n",
        "\n",
        "    score_1, error_1 = api.request(text_success)[0]\n",
        "    assert score_1 and not error_1\n",
        "\n",
        "    score_2, error_2 = api.request(text_error)[0]\n",
        "    assert not score_2 and isinstance(error_2, HttpError)\n",
        "\n",
        "    multi_score, multi_error = zip(*api.request([text_success, text_error]))\n",
        "    assert multi_score == (score_1, score_2)\n",
        "    assert tuple(map(str, multi_error)) == tuple(map(str, (error_1, error_2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUMz5V4lXmXF"
      },
      "source": [
        "def make_generations_col(generations, responses):\n",
        "    for generation, response in zip(generations, responses):\n",
        "        if response['response']:\n",
        "            response = unpack_scores(response['response'])[0]\n",
        "        else:\n",
        "            response = {x: None for x in PERSPECTIVE_API_ATTRIBUTES_LOWER}\n",
        "        yield {'text': generation, **response}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4YQdK93Xalh"
      },
      "source": [
        "def collate(dataset: Optional[pd.DataFrame], generations: List[str], responses: Iterable[Dict[str, Any]],\n",
        "            output_file: str):\n",
        "    generations_col_iter = make_generations_col(generations, responses)\n",
        "    if dataset is None:\n",
        "        generations_col = list(tqdm(generations_col_iter, total=len(generations), desc='Collating files'))\n",
        "        dataset = pd.DataFrame(generations_col)\n",
        "    else:\n",
        "        assert len(generations) % len(dataset) == 0\n",
        "        n = len(generations) // len(dataset)\n",
        "        print(f\"Detected samples per prompt:\", n)\n",
        "        generations_col = list(tqdm(batchify(generations_col_iter, n), total=len(dataset), desc='Collating files'))\n",
        "        dataset['generations'] = generations_col\n",
        "\n",
        "    dataset.to_json(output_file, orient='records', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogN1I240eoHQ"
      },
      "source": [
        "#Set up prompts which only contains a end of sentenc character for unprompted generation for GPT1\r\n",
        "from transformers import pipeline\r\n",
        "dataset = pd.read_json('/gdrive/My Drive/517finalP/realtoxicityprompts-data/eosgpt1.jsonl', lines=True)\r\n",
        "prompts = pd.json_normalize(dataset['prompt'])['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kVcvgLYa9WY",
        "outputId": "a5f10100-9779-491b-e9b0-fd3c90d7e3ab"
      },
      "source": [
        "print(prompts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       . \n",
            "1       . \n",
            "2       . \n",
            "3       . \n",
            "4       . \n",
            "        ..\n",
            "9995    . \n",
            "9996    . \n",
            "9997    . \n",
            "9998    . \n",
            "9999    . \n",
            "Name: text, Length: 10000, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204,
          "referenced_widgets": [
            "e7aee9eb733c4537b0e78795bdf582a7",
            "b591e65dc6c84fdbbf27a7d942a2f414",
            "0f9f2c06a77a40b0908db938e04e5e6f",
            "0a7987c7fe6f4e8e8121618a0552cfd7",
            "1d657bf3a99a4657a6e198812dab54b7",
            "648b9e882a1543bbbbf041d34daddaa6",
            "510c828c02d44eed92b58f9c9dd4ad8f",
            "2082377cdc1541bc8b6e50a403558b9d",
            "6d2c18127d5a46e59f652c1d52b1020c",
            "3228b392207947989b1eecf243583628",
            "e2e3fb1049d94c70a93e4acb9b3d7ef3",
            "98c378636d7b4099a5fcf2ab530e85dd",
            "06f3b4a7d7f84571a891c27c8bb95617",
            "dd7e3533fc9d4f348a8aaa7cce4b78a1",
            "e6c1556b3af546efb9b46e11b2b518dd",
            "09dac1e26ebc421e8c1a367324d88e5c"
          ]
        },
        "id": "3wjgdqtxH5g_",
        "outputId": "807879d8-c296-4827-a489-a91b4643e782"
      },
      "source": [
        "#Generate unprompted sentece with GPT1 model\n",
        "output_dir = Path('/gdrive/My Drive/517finalP/realtoxicityprompts-data/unpromptedgpt111111000')\n",
        "output_dir.mkdir(exist_ok=False)\n",
        "generations_file = output_dir / 'generations.jsonl'\n",
        "perspective_file = output_dir / 'perspective.jsonl'\n",
        "output_file = output_dir / f'{\"unprompted\"}_gens_{\"gpt\"}.jsonl'\n",
        "\n",
        "# Create perspective worker thread\n",
        "perspective = PerspectiveWorker(out_file=perspective_file,\n",
        "                                total=len(prompts) * 1,\n",
        "                                rate_limit=1)\n",
        "    \n",
        "#generations_iter = ctrl(prompts=prompts, max_len=20, num_samples=25, ctrl_code=\"Links\", model_name_or_path=\"ctrl\", out_file=generations_file)\n",
        "generations_iter = openai_gpt(prompts=prompts, max_len=20, num_samples=1, model_name_or_path=\"openai-gpt\", out_file=generations_file)\n",
        "# Generate and collate perspective scores\n",
        "generations = []\n",
        "for i, gen in enumerate(generations_iter):\n",
        "    generations.append(gen)\n",
        "    perspective(f'generation-{i}', gen)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "perspective.stop()\n",
        "print('Finished generation and perspective scoring!')\n",
        "\n",
        "print('Collating output files')\n",
        "collate(dataset, generations, load_jsonl(perspective_file), output_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created pipeline with model: OpenAIGPTLMHeadModel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7aee9eb733c4537b0e78795bdf582a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Generation', layout=Layout(flex='2'), max=10000.0, style=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Waiting for Perspective to finish...\n",
            "Finished generation and perspective scoring!\n",
            "Collating output files\n",
            "Detected samples per prompt: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d2c18127d5a46e59f652c1d52b1020c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Collating files', max=10000.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o79AbVqS70g"
      },
      "source": [
        "#Generate output file for unprompted GPT1 \r\n",
        "dataset = pd.read_json('/gdrive/My Drive/517finalP/realtoxicityprompts-data/unpromptedgpt111111000/unprompted_gens_gpt.jsonl', lines=True)\r\n",
        "dataset = dataset['generations']\r\n",
        "f = open('/gdrive/My Drive/517finalP/realtoxicityprompts-data/unprompted_gen_gpt1.jsonl', 'w')\r\n",
        "count = 0\r\n",
        "for line in dataset:\r\n",
        "  json.dump(line[0], f)\r\n",
        "  f.write(\"\\n\")\r\n",
        "f.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqmRC8UDjRBf"
      },
      "source": [
        "#Set up prompts which only contains a end of sentenc character for unprompted generation for GPT2\r\n",
        "from transformers import pipeline\r\n",
        "dataset = pd.read_json('/gdrive/My Drive/517finalP/realtoxicityprompts-data/eosgpt2.jsonl', lines=True)\r\n",
        "prompts = pd.json_normalize(dataset['prompt'])['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242,
          "referenced_widgets": [
            "d8823c99c8a54287a4a51845aeb3279f",
            "f034202c5c534c7bac07adbb2b998533",
            "6f45679fcada464fbf7024377d636bc8",
            "b31fd813574e4f43a284c58b5a0116e5",
            "32a2f95529164e0b8a27c7af039b2cb8",
            "1dcee0d7357d41b9949e8f23e2c32b5a",
            "271c4c7895b9411ca3878b6503382305",
            "ba548ffd65aa4c12a0c77b3d58b8fd0f",
            "fbd3fd5a707d42a9a3f13cb9627760b0",
            "9742e259b8174d6a9eba2973019dd5f7",
            "e487c7fa96f94c5b9b732f61039f25f5",
            "46593b2b9be44dd28ae373803c416a9e",
            "e2c7695576ea4098af6af7f5cb1e293c",
            "774c436047a0472eaf79184c2a0f009e",
            "92d31676cc3d43d48baa1b56d958b6fc",
            "1f6040b44bc14c67b14b4ceb7ea38c86"
          ]
        },
        "id": "g2yQJm3ucTHh",
        "outputId": "378064e9-7e86-48e4-fa4e-d008ba407f55"
      },
      "source": [
        "#Generate unprompted sentece with GPT1 model\n",
        "output_dir = Path('/gdrive/My Drive/517finalP/realtoxicityprompts-data/unpromptedgpt222222')\n",
        "output_dir.mkdir(exist_ok=False)\n",
        "generations_file = output_dir / 'generations.jsonl'\n",
        "perspective_file = output_dir / 'perspective.jsonl'\n",
        "output_file = output_dir / f'{\"unprompted\"}_gens_{\"gpt2\"}.jsonl'\n",
        "\n",
        "# Create perspective worker thread\n",
        "perspective = PerspectiveWorker(out_file=perspective_file,\n",
        "                                total=len(prompts) * 25,\n",
        "                                rate_limit=1)\n",
        "    \n",
        "#generations_iter = ctrl(prompts=prompts, max_len=20, num_samples=25, ctrl_code=\"Links\", model_name_or_path=\"ctrl\", out_file=generations_file)\n",
        "generations_iter = gpt2(prompts=prompts, max_len=20, num_samples=1, batch_size = 32, model_name_or_path=\"gpt2\", out_file=generations_file)\n",
        "# Generate and collate perspective scores\n",
        "generations = []\n",
        "for i, gen in enumerate(generations_iter):\n",
        "    generations.append(gen)\n",
        "    perspective(f'generation-{i}', gen)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "perspective.stop()\n",
        "print('Finished generation and perspective scoring!')\n",
        "\n",
        "print('Collating output files')\n",
        "collate(dataset, generations, load_jsonl(perspective_file), output_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8823c99c8a54287a4a51845aeb3279f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='GPT-2 Generation', layout=Layout(flex='2'), max=313.0, st…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Waiting for Perspective to finish...\n",
            "Finished generation and perspective scoring!\n",
            "Collating output files\n",
            "Detected samples per prompt: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbd3fd5a707d42a9a3f13cb9627760b0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Collating files', max=10000.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pUFqXnQcYKZ"
      },
      "source": [
        "#Generate output file for unprompted GPT2 \r\n",
        "dataset = pd.read_json('/gdrive/My Drive/517finalP/realtoxicityprompts-data/unpromptedgpt222222/unprompted_gens_gpt2.jsonl', lines=True)\r\n",
        "dataset = dataset['generations']\r\n",
        "f = open('/gdrive/My Drive/517finalP/realtoxicityprompts-data/unprompted_gen_gpt2.jsonl', 'w')\r\n",
        "count = 0\r\n",
        "for line in dataset:\r\n",
        "  json.dump(line[0], f)\r\n",
        "  f.write(\"\\n\")\r\n",
        "f.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl59pwyFctFa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSit5tezc1zP"
      },
      "source": [
        "#Set up prompts which only contains a end of sentence character for unprompted generation for CTRL\r\n",
        "from transformers import pipeline\r\n",
        "dataset = pd.read_json('/gdrive/My Drive/517finalP/realtoxicityprompts-data/eos.jsonl', lines=True)\r\n",
        "prompts = pd.json_normalize(dataset['prompt'])['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618,
          "referenced_widgets": [
            "e636e6a06fab4529b2ead56b9650b7a0",
            "9ec163fd22c34527b81f33c124361647",
            "587887ed31d348eba4f0f59decaf392b",
            "4fdf116ca330463e94c6a06feb99220d",
            "4906492d0c9b4f5bbe75c07170c31794",
            "cc4eb119852e41f3af34a8e49a046ba9",
            "fc6c6aa4409f48c2a5b887010ff2f2ed",
            "3d037ad7c3834f5ea2cba600bd26e928",
            "37d6b6ffe16346d1a0fe73473b9c4ae7",
            "da875683cec04f8393d118dae6ac2c2e",
            "c557038c20094c63903236a7a4385100",
            "84598c938cdc4ecbaf0b9401bd5b785a",
            "de2dcd60b0d941da91bdec65f87f43dc",
            "891db7fe8747449289a685cb2826d0a1",
            "c276d5e662a647dfb55d490313d18a76",
            "e437990dc6f94658a1f7c58df5b1e5ce",
            "2606ce8ef711408eb0a723a48f57cb0b",
            "c65ff1c6dd2d45c8b7ee1ae1395c22cd",
            "690a19dcda86499dabcbbe15916f048d",
            "dd56a68a279d4b20be5891ca01044e3a",
            "e3001d0d03cb4ababa82e5a8332b4387",
            "8252c38f901e4674969f78f368fd7d85",
            "1c37334c5cf34f6a884c80cfc0397ea0",
            "58e5d456cfa2456494d3c45cf99f8c0e",
            "50ea581721ce471ba294b173dab2ae02",
            "7984fb0da683473e9de7912434d6a0ea",
            "a9591800568e449a9eccf36287486bd3",
            "75e4c5d779d84259aa29a98665c4645d",
            "3bc5f84f52614176a8d0e3e06387c690",
            "bb52603190e041d5bcdeecbe8472bd75",
            "3ace364197f541b48fe0a0c004bd2d06",
            "e3c46d7384704bfa8872f135329ec1ab",
            "c739c97887644c798e274897aca59166",
            "ae41cd724e2346cba3581a22d8231fc5",
            "c7d1c68cd30e475c80bdccae33dcd864",
            "df3d94ad9e00462d9e2f493336cc24d1",
            "a088f894715b4250b1d05ac271141b74",
            "d18d088742a64333b5331b5012f70962",
            "bbaa917c463f430aad1bba379ac5c061",
            "25cdb726a9d94952b5b56f94a96d4c18",
            "dc89bc98698b4f0a9d2d390a80f9d244",
            "f41994363be84c7280df83d62ca675a8",
            "6396ba6e0936437785ad4ea68485d6a7",
            "1ccb24a008a849ba86e6ec81bb561131",
            "3ba40fa850424b1ab3b22133ab2da68e",
            "2a8a0efe64f947c19c8979adb145a0e6",
            "931cd28e2a0f47e5bc70687aaf0248dd",
            "86dd14d87f84491494718008529b86cc"
          ]
        },
        "id": "xDk0qyTsc3zS",
        "outputId": "7726faf7-b50f-48de-c26c-11827fad2714"
      },
      "source": [
        "#Generate unprompted sentece with GPT1 model\n",
        "output_dir = Path('/gdrive/My Drive/517finalP/realtoxicityprompts-data/unpromptedctrl11')\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "generations_file = output_dir / 'generations.jsonl'\n",
        "perspective_file = output_dir / 'perspective.jsonl'\n",
        "output_file = output_dir / f'{\"unprompted\"}_gens_{\"ctrl\"}.jsonl'\n",
        "\n",
        "# Create perspective worker thread\n",
        "perspective = PerspectiveWorker(out_file=perspective_file,\n",
        "                                total=len(prompts) * 1,\n",
        "                                rate_limit=1)\n",
        "    \n",
        "generations_iter = ctrl(prompts=prompts, max_len=20, num_samples=1, ctrl_code=\"Links\", model_name_or_path=\"ctrl\", out_file=generations_file)\n",
        "#generations_iter = gpt2(prompts=prompts, max_len=20, num_samples=1, batch_size = 32, model_name_or_path=\"gpt2\", out_file=generations_file)\n",
        "# Generate and collate perspective scores\n",
        "generations = []\n",
        "for i, gen in enumerate(generations_iter):\n",
        "    generations.append(gen)\n",
        "    perspective(f'generation-{i}', gen)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "perspective.stop()\n",
        "print('Finished generation and perspective scoring!')\n",
        "\n",
        "print('Collating output files')\n",
        "collate(dataset, generations, load_jsonl(perspective_file), output_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       Links \n",
            "1       Links \n",
            "2       Links \n",
            "3       Links \n",
            "4       Links \n",
            "         ...  \n",
            "9995    Links \n",
            "9996    Links \n",
            "9997    Links \n",
            "9998    Links \n",
            "9999    Links \n",
            "Name: text, Length: 10000, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e636e6a06fab4529b2ead56b9650b7a0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=611.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37d6b6ffe16346d1a0fe73473b9c4ae7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=6552025106.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2606ce8ef711408eb0a723a48f57cb0b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2049344.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50ea581721ce471ba294b173dab2ae02",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1004909.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Created pipeline with model: CTRLLMHeadModel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c739c97887644c798e274897aca59166",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Generation', layout=Layout(flex='2'), max=10000.0, style=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Waiting for Perspective to finish...\n",
            "Finished generation and perspective scoring!\n",
            "Collating output files\n",
            "Detected samples per prompt: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc89bc98698b4f0a9d2d390a80f9d244",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Collating files', max=10000.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TqJvjojc6aZ"
      },
      "source": [
        "#Generate output file for unprompted GPT2 \r\n",
        "dataset = pd.read_json('/gdrive/My Drive/517finalP/realtoxicityprompts-data/unpromptedctrl11/unprompted_gens_ctrl.jsonl', lines=True)\r\n",
        "dataset = dataset['generations']\r\n",
        "f = open('/gdrive/My Drive/517finalP/realtoxicityprompts-data/unprompted_gen_ctrl.jsonl', 'w')\r\n",
        "count = 0\r\n",
        "for line in dataset:\r\n",
        "  json.dump(line[0], f)\r\n",
        "  f.write(\"\\n\")\r\n",
        "f.close()"
      ],
      "execution_count": 5,
      "outputs": []
    }
  ]
}